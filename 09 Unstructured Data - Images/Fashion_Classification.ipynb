{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification using Neural Networks\n",
    "In this lesson, we will learn more about unstructured inputs: images.\n",
    "\n",
    "In this notebook, we will use a public dataset, the Fashion MNIST data set. This dataset contains 60,000 images (28 x 28) of 10 different types of clothes.\n",
    "\n",
    "In addition, the test set contains 10,000 similar images.\n",
    "\n",
    "### Step 1: Load and explore data\n",
    "Using a python package by tensorflow (keras), we can directly import the data into our local environment.\n",
    "\n",
    "We can use matplotlib to visualise some of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(type(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = train_images[0]\n",
    "image_pixels = image.reshape(28, 28)\n",
    "plt.subplot(131)\n",
    "plt.imshow(image_pixels)\n",
    "plt.axis('off')\n",
    "\n",
    "print(train_labels[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 6000],\n",
       "       [   1, 6000],\n",
       "       [   2, 6000],\n",
       "       [   3, 6000],\n",
       "       [   4, 6000],\n",
       "       [   5, 6000],\n",
       "       [   6, 6000],\n",
       "       [   7, 6000],\n",
       "       [   8, 6000],\n",
       "       [   9, 6000]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "(unique, counts) = np.unique(train_labels, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "frequencies\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classes are well-balanced\n",
    "\n",
    "0 = Top / T-shirt<br>\n",
    "1 = Trouser<br>\n",
    "2 = Pullover<br>\n",
    "3 = Dress<br>\n",
    "4 = Coat<br>\n",
    "5 = Sandal<br>\n",
    "6 = Shirt<br>\n",
    "7 = Sneaker<br>\n",
    "8 = Bag<br>\n",
    "9 = Ankle Boot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class Classification\n",
    "\n",
    "We will try using the architecture we learnt about last time - Artificial Neural Networks (ANNs)\n",
    "\n",
    "We will also look at a much more efficient archiecture: Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train_images = train_images.reshape(60000, 28, 28, 1)\n",
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Neural Network (ANN)\n",
    "\n",
    "This is the architecture we learnt about last lesson. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = keras.models.Sequential()\n",
    "#Input layer\n",
    "ann.add(keras.layers.Flatten(input_shape=[28, 28, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hidden Layer 1\n",
    "ann.add(keras.layers.Dense(50, activation='relu'))\n",
    "\n",
    "#Hidden Layer 2\n",
    "ann.add(keras.layers.Dense(25, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output layer (with softmax)\n",
    "ann.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_11 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 50)                39250     \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 25)                1275      \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 10)                260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40785 (159.32 KB)\n",
      "Trainable params: 40785 (159.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ann.build((28, 28, 1))\n",
    "ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2ccd18310>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(train_images, train_labels, epochs=20, verbose=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN)\n",
    "\n",
    "This is the most commonly used architecture for image classification. \n",
    "\n",
    "Learn more about this architecture in this lesson's video: https://youtu.be/DbClQQZujxA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = keras.models.Sequential()\n",
    "#4 Convolutional Filters\n",
    "cnn.add(keras.layers.Conv2D(4, (4, 4), strides=2, activation='relu', input_shape=(28, 28, 1)))\n",
    "cnn.add(keras.layers.MaxPooling2D((2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 13, 13, 4)         68        \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 6, 6, 4)           0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 144)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 68 (272.00 Byte)\n",
      "Trainable params: 68 (272.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.add(keras.layers.Flatten())\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 13, 13, 4)         68        \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 6, 6, 4)           0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 144)               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 50)                7250      \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 25)                1275      \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 10)                260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8853 (34.58 KB)\n",
      "Trainable params: 8853 (34.58 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.add(keras.layers.Dense(50, activation='relu'))\n",
    "cnn.add(keras.layers.Dense(25, activation='relu'))\n",
    "cnn.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of CNNs\n",
    "\n",
    "Despite having a more complex architecture = more complex patterns, the number of parameters in a CNN are fewer (less than one-fourth).\n",
    "\n",
    "|Architecture|HL 1|HL 2|Parameters|\n",
    "|------------|----|----|----------|\n",
    "|ANN|50|25|40785|\n",
    "|CNN|50|25|8853|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29ee9a190>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(train_images, train_labels, epochs=20, verbose=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 646us/step - loss: 1.6052 - accuracy: 0.5947\n",
      "ANN Accuracy: 0.5946999788284302\n"
     ]
    }
   ],
   "source": [
    "ann_score = ann.evaluate(test_images, test_labels)\n",
    "print(f\"ANN Accuracy: {ann_score[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 617us/step - loss: 0.5009 - accuracy: 0.8349\n",
      "CNN Accuracy: 0.8349000215530396\n"
     ]
    }
   ],
   "source": [
    "cnn_score = cnn.evaluate(test_images, test_labels)\n",
    "print(f\"CNN Accuracy: {cnn_score[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we can determine that for tasks related to images, CNNs are much more efficient and perform much better than ANNs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
